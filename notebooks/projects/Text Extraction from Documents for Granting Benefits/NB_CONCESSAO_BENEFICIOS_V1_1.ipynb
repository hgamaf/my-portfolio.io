{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [D&A Conexão Digital] Classificador de documentos para concessão de benefícios"
      ],
      "metadata": {
        "id": "6rO3sZd7SOkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber PyPDF2 pdf2image pytesseract yake language_tool_python pyspellchecker\n",
        "!sudo apt install tesseract-ocr\n",
        "!apt-get install tesseract-ocr-por #tesseract em pt-br (por)\n",
        "!apt-get install -y poppler-utils #necessária para conveter pdf -> imagem\n",
        "!pip install Pillow==9.5.0 #deve-se utilizar essa versão para: pdf -> imagem"
      ],
      "metadata": {
        "id": "IqnS3OfeSQRp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19ccc70b-2d25-4bcd-d024-e13f67034397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.9.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting yake\n",
            "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting language_tool_python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=9.1 (from pdfplumber)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Wand>=0.6.10 (from pdfplumber)\n",
            "  Downloading Wand-0.6.11-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.6/143.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (40.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.8.10)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake) (8.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yake) (1.22.4)\n",
            "Collecting segtok (from yake)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.1)\n",
            "Collecting jellyfish (from yake)\n",
            "  Downloading jellyfish-0.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segtok->yake) (2022.10.31)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: Wand, segtok, pyspellchecker, PyPDF2, Pillow, jellyfish, yake, pytesseract, pdf2image, language_tool_python, pdfminer.six, pdfplumber\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed Pillow-9.5.0 PyPDF2-3.0.1 Wand-0.6.11 jellyfish-0.11.2 language_tool_python-2.7.1 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.9.0 pyspellchecker-0.7.2 pytesseract-0.3.10 segtok-1.5.11 yake-0.4.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 4,850 kB of archives.\n",
            "After this operation, 16.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1 [1,598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr amd64 4.1.1-2build2 [262 kB]\n",
            "Fetched 4,850 kB in 1s (5,202 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2build2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2build2) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2build2) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-por\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 856 kB of archives.\n",
            "After this operation, 1,998 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-por all 1:4.00~git30-7274cfa-1 [856 kB]\n",
            "Fetched 856 kB in 1s (1,309 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-por.\n",
            "(Reading database ... 122588 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-por_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
            "Unpacking tesseract-ocr-por (1:4.00~git30-7274cfa-1) ...\n",
            "Setting up tesseract-ocr-por (1:4.00~git30-7274cfa-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 174 kB of archives.\n",
            "After this operation, 754 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 poppler-utils amd64 0.86.1-0ubuntu1.1 [174 kB]\n",
            "Fetched 174 kB in 0s (382 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 122592 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.86.1-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking poppler-utils (0.86.1-0ubuntu1.1) ...\n",
            "Setting up poppler-utils (0.86.1-0ubuntu1.1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow==9.5.0 in /usr/local/lib/python3.10/dist-packages (9.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lista de idiomas do tesseract\n",
        "!tesseract --list-langs"
      ],
      "metadata": {
        "id": "yuZ6_rPMSSuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b310234b-2978-485b-8435-6a1e030daf81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of available languages (3):\n",
            "eng\n",
            "osd\n",
            "por\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir tessdata\n",
        "!wget -O ./tessdata/por.traineddata https://github.com/tesseract-ocr/tessdata/blob/main/por.traineddata?raw=true\n",
        "config_tesseract = '--tessdata-dir tessdata'"
      ],
      "metadata": {
        "id": "eG5QhWftSVf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9385bb-40e4-4845-cc23-10456567486c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-09 12:56:12--  https://github.com/tesseract-ocr/tessdata/blob/main/por.traineddata?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/tesseract-ocr/tessdata/raw/main/por.traineddata [following]\n",
            "--2023-06-09 12:56:12--  https://github.com/tesseract-ocr/tessdata/raw/main/por.traineddata\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tesseract-ocr/tessdata/main/por.traineddata [following]\n",
            "--2023-06-09 12:56:12--  https://raw.githubusercontent.com/tesseract-ocr/tessdata/main/por.traineddata\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15336931 (15M) [application/octet-stream]\n",
            "Saving to: ‘./tessdata/por.traineddata’\n",
            "\n",
            "./tessdata/por.trai 100%[===================>]  14.63M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-06-09 12:56:12 (332 MB/s) - ‘./tessdata/por.traineddata’ saved [15336931/15336931]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import pdfplumber\n",
        "import cv2 #opencv\n",
        "from collections import Counter\n",
        "import pytesseract\n",
        "\n",
        "import glob\n",
        "\n",
        "from yake import KeywordExtractor\n",
        "from spellchecker import SpellChecker\n",
        "import language_tool_python\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#import matplotlib.pyplot as plt foi utilizando apenas para testes"
      ],
      "metadata": {
        "id": "wb4lpd7jSYiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pytesseract.get_tesseract_version())"
      ],
      "metadata": {
        "id": "IXe4xtYEAxh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b5a9b7-7faa-4934-df08-52ee3fb6567e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções para o output do OCR"
      ],
      "metadata": {
        "id": "X1B4dz7fSlfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def padrao_nome(texto):\n",
        "    regex = r'(?i)(?:NOME|Nome da Pessoa Física|COMPLETO|NAME|NOMBRE)[:\\s]*([^\\n,]+)'\n",
        "    resultado = re.search(regex, texto, re.IGNORECASE)\n",
        "    return resultado.group(1) if resultado else None\n",
        "\n",
        "def fantasia_nome(texto):\n",
        "    regex = r'(?i)(?:NOME EMPRESARIAL|NOME FANTASIA| Nome de Fantansia)[:\\s]*([^\\n,]+)'\n",
        "    resultado = re.search(regex, texto, re.IGNORECASE)\n",
        "    return resultado.group(1) if resultado else None\n",
        "\n",
        "def padrao_cpf(texto):\n",
        "    regex = r'CPF:\\s*(\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2})'\n",
        "    resultado = re.search(regex, texto, re.IGNORECASE)\n",
        "    return resultado.group(1) if resultado else None\n",
        "\n",
        "def padrao_cnpj(texto):\n",
        "    regex = r'CNPJ:\\s*(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})'\n",
        "    resultado = re.search(regex, texto, re.IGNORECASE)\n",
        "    return resultado.group(1) if resultado else None\n",
        "\n",
        "def padrao_situacao_cadastral(texto):\n",
        "    regex = r'(?:Situação Cadastral:)\\s*(.*)\\n'\n",
        "    resultado = re.search(regex, texto, re.IGNORECASE)\n",
        "    return resultado.group(1) if resultado else None\n",
        "\n",
        "def padrao_data_nascimento(texto):\n",
        "    regex = r'(?:Data de Nascimento:)\\s*(.*)\\n'\n",
        "    resultado = re.search(regex, texto, re.IGNORECASE)\n",
        "    return resultado.group(1) if resultado else None\n",
        "\n",
        "import re\n",
        "\n",
        "def find_uf(texto):\n",
        "    uf_list = [\"AC\", \"AL\", \"AP\", \"AM\", \"BA\", \"CE\", \"DF\", \"ES\", \"GO\", \"MA\", \"MT\", \"MS\", \"MG\", \"PA\", \"PB\", \"PR\", \"PE\", \"PI\", \"RJ\", \"RN\", \"RS\", \"RO\", \"RR\", \"SC\", \"SP\", \"SE\", \"TO\"]\n",
        "    for uf in uf_list:\n",
        "        if re.search(r'\\b' + uf + r'\\b', texto):\n",
        "            return uf\n",
        "    return None\n",
        "\n",
        "def buscar_siglas(nome_arquivo):\n",
        "    siglas = ['CCIR', 'CNPJ', 'CPF', 'CTRU', 'IE', 'IRR', 'ITR', 'NIRF',\n",
        "              'PRMA', 'PRONAF', 'RG', 'RURL', 'VIDA', 'RMMV', 'OUTA',\n",
        "              'LIAM', 'AUTODECLARAÇÃO', 'ENBE', 'NB', 'NIS', 'PRIC',\n",
        "              'PRIE', 'ITR', 'CONS']\n",
        "\n",
        "    for sigla in siglas:\n",
        "        if sigla in nome_arquivo:\n",
        "            return sigla\n",
        "\n",
        "    return None\n",
        "\n",
        "def atribuir_distribuidora(nome_arquivo):\n",
        "    if nome_arquivo.startswith('1'):\n",
        "        return 'NCLB'\n",
        "    elif nome_arquivo.startswith('2'):\n",
        "        return 'NPER'\n",
        "    elif nome_arquivo.startswith('3'):\n",
        "        return 'NCSR'\n",
        "    else:\n",
        "        return 'None'"
      ],
      "metadata": {
        "id": "syyAVKgdSdoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converter de .pdf para .jpeg e aplicar o OCR"
      ],
      "metadata": {
        "id": "RbkrAybxSrWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o caminho do arquivo PDF.\n",
        "pdf_path = r\"/content/sample_data/RG_HENRIQUE.pdf\"\n",
        "\n",
        "def convert_pdf_to_image(pdf_path):\n",
        "    # Converte pdf para imagem.\n",
        "    images = convert_from_path(pdf_path)\n",
        "    return images\n",
        "\n",
        "def extrair_informacoes_imagem(image, pdf_path):\n",
        "    # Converte a imagem PIL em um array NumPy\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Converter a imagem para escala de cinza\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Realizar o OCR com pytesseract\n",
        "    texto = pytesseract.image_to_string(gray, lang='por')\n",
        "\n",
        "    # Aqui, estou presumindo que você já definiu funções como \"padrao_nome\", \"padrao_cpf\", etc.\n",
        "    # Retornar as informações encontradas (ou None se não encontradas)\n",
        "    return {\n",
        "        'NOME_ARQ': os.path.basename(pdf_path),\n",
        "        'DISTRIBUIDORA': atribuir_distribuidora(os.path.basename(pdf_path)),\n",
        "        'DECLARADO': buscar_siglas(os.path.basename(pdf_path)),\n",
        "        'NOME_CPF': padrao_nome(texto),\n",
        "        'NOME_CNPJ': fantasia_nome(texto),\n",
        "        'CPF': padrao_cpf(texto),\n",
        "        'CNPJ': padrao_cnpj(texto),\n",
        "        'SITU_CAD': padrao_situacao_cadastral(texto),\n",
        "        'DTA_NASC': padrao_data_nascimento(texto),\n",
        "        'UF': find_uf(texto),\n",
        "        'TEXTO': texto  # Adicionando o texto extraído como uma nova coluna\n",
        "    }\n",
        "\n",
        "# Lista para armazenar as informações extraídas\n",
        "informacoes_lista = []\n",
        "\n",
        "# Converter o PDF em imagens\n",
        "images = convert_pdf_to_image(pdf_path)\n",
        "\n",
        "# Loop para processar cada imagem\n",
        "for image in images:\n",
        "    # Extrair as informações da imagem\n",
        "    informacoes = extrair_informacoes_imagem(image, pdf_path)\n",
        "\n",
        "    # Adicionar as informações à lista\n",
        "    informacoes_lista.append(informacoes)\n",
        "\n",
        "# Criar um DataFrame a partir da lista de informações\n",
        "DF = pd.DataFrame(informacoes_lista)"
      ],
      "metadata": {
        "id": "RZhDZNTQOum1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o caminho do diretório.\n",
        "dir_path = '/content/sample_data/exemplos'\n",
        "\n",
        "# Lista todos os arquivos no diretório.\n",
        "files = os.listdir(dir_path)\n",
        "\n",
        "# Filtra apenas os arquivos PDF.\n",
        "pdf_files = [file for file in files if file.endswith('.pdf') or file.endswith('.PDF')]\n",
        "\n",
        "def convert_pdf_to_images(pdf_path):\n",
        "    # Converte pdf para imagens.\n",
        "    images = convert_from_path(pdf_path)\n",
        "    return images\n",
        "\n",
        "def extrair_informacoes_imagem(image):\n",
        "    # Converte a imagem PIL em um array NumPy\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Converter a imagem para escala de cinza\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Realizar o OCR com pytesseract\n",
        "    texto = pytesseract.image_to_string(gray, lang='por')\n",
        "\n",
        "    # Retornar as informações encontradas (ou None se não encontradas)\n",
        "    return {\n",
        "        'NOME_ARQ': os.path.basename(pdf_path),\n",
        "        'DISTRIBUIDORA': atribuir_distribuidora(os.path.basename(pdf_path)),\n",
        "        'DECLARADO': buscar_siglas(os.path.basename(pdf_path)),\n",
        "        'NOME_CPF': padrao_nome(texto),\n",
        "        'NOME_CNPJ': fantasia_nome(texto),\n",
        "        'CPF': padrao_cpf(texto),\n",
        "        'CNPJ': padrao_cnpj(texto),\n",
        "        'SITU_CAD': padrao_situacao_cadastral(texto),\n",
        "        'DTA_NASC': padrao_data_nascimento(texto),\n",
        "        'UF': find_uf(texto),\n",
        "        'TEXTO': texto  # Adicionando o texto extraído como uma nova coluna\n",
        "    }\n",
        "\n",
        "# Lista para armazenar as informações extraídas\n",
        "informacoes_lista = []\n",
        "\n",
        "# Loop para processar cada pdf\n",
        "for pdf_file in pdf_files:\n",
        "    # Caminho completo do pdf\n",
        "    pdf_path = os.path.join(dir_path, pdf_file)\n",
        "\n",
        "    # Converter o PDF em imagens\n",
        "    images = convert_pdf_to_images(pdf_path)\n",
        "\n",
        "    # Loop para processar cada imagem\n",
        "    for image in images:\n",
        "        # Extrair as informações da imagem\n",
        "        informacoes = extrair_informacoes_imagem(image)\n",
        "\n",
        "        # Adicionar as informações à lista\n",
        "        informacoes_lista.append(informacoes)\n",
        "\n",
        "# Criar um DataFrame a partir da lista de informações\n",
        "DF = pd.DataFrame(informacoes_lista)"
      ],
      "metadata": {
        "id": "zL3DyXZDSxzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extrair as KEYWORDS do TEXTO (output do OCR)"
      ],
      "metadata": {
        "id": "wppEEvQETFfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para extrair as palavras-chave\n",
        "\n",
        "kw_extractor = KeywordExtractor(lan='pt', n=1, top=30)\n",
        "\n",
        "\n",
        "def extrair_palavras_chave(texto):\n",
        "    keywords = kw_extractor.extract_keywords(texto)\n",
        "    return [keyword for keyword, _ in keywords]\n",
        "\n",
        "# Aplicar a função para extrair as palavras-chave a cada linha do DataFrame\n",
        "DF['KEYWORDS'] = DF['TEXTO'].apply(extrair_palavras_chave)"
      ],
      "metadata": {
        "id": "KDqXv0xiTKAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mensurar a qualidade do OCR através das métricas CER e WER (Character Error Rate & Word Error Rate)"
      ],
      "metadata": {
        "id": "ReqozOdCTP8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Não é possível mensurar a qualidade do OCR a partir das métricas de CER e WER pois é necessário realizar a comparação com uma referência (texto original)."
      ],
      "metadata": {
        "id": "kkfuifz4TRgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LINK: https://towardsdatascience.com/evaluating-ocr-output-quality-with-character-error-rate-cer-and-word-error-rate-wer-853175297510"
      ],
      "metadata": {
        "id": "kFy2B-3yTZOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verificar se existem as palavras no dicionário**: Cada palavra no texto do OCR é comparada com um dicionário do idioma PT-BR. Se uma grande porcentagem das palavras não estiver no dicionário, isso pode indicar um problema com o OCR."
      ],
      "metadata": {
        "id": "86PtESwoTdUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spell = SpellChecker(language='pt')\n",
        "\n",
        "def calculate_quality(text):\n",
        "    if len(text) == 0:\n",
        "        return None\n",
        "    unknown_words = spell.unknown(text)\n",
        "    return len(unknown_words) / len(text)\n",
        "\n",
        "DF['COMPARACAO_DICIONARIO'] = DF['KEYWORDS'].apply(calculate_quality)"
      ],
      "metadata": {
        "id": "vlzRDqimTdp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Um valor de 0 indica que todas as palavras da lista são reconhecidas pelo corretor ortográfico, ou seja, todas as palavras existem no dicionário da língua definida (neste caso, português). Isso sugere que as palavras são todas ortograficamente corretas.\n",
        "\n",
        "*   Um valor de 1 indica que nenhuma palavra na lista é reconhecida pelo corretor ortográfico. Isso pode sugerir que as palavras estão todas ortograficamente incorretas, ou que a lista contém palavras específicas ou técnicas que não estão presentes no dicionário padrão."
      ],
      "metadata": {
        "id": "AHL_KHRvTkDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str(DF.KEYWORDS[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BMymwoZFO8NX",
        "outputId": "0190dbac-15ee-4818-b1e9-02691992e2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['FONTES', 'SANTOS', 'GAMA', 'LOS', 'Poa', 'Tra', 'Sera', 'DEUSDEDIT', 'CAMPINA', 'nen', 'esti', 'enlisafaaesaa', 'coca', 'ADALGISA', 'MARIA', 'GRANDE-PB', 'NASC', 'FERRAZ', 'DIRETOR', 'IACOUELINE', 'OLIVEIRA', 'ASSINATURA', '096995196-57', 'soco']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contagem das palavras mais frequentes para cada tipo e documento (COLUNA:DECLARADO)"
      ],
      "metadata": {
        "id": "0oAtBBYlTsHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testar as palavras únicas que aparecem somente em cada um dos documentos."
      ],
      "metadata": {
        "id": "EnXL4zSpTwFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar o extrator de palavras-chave\n",
        "kw_extractor = KeywordExtractor(lan='pt', n=1, top=10)\n",
        "\n",
        "# Agrupar o DataFrame por tipo de documento\n",
        "GRUPOS = DF.groupby('DECLARADO')\n",
        "\n",
        "# Para cada grupo, concatenar todo o texto e extrair as palavras-chave\n",
        "palavras_chave = {nome: set(palavra for palavra, _ in kw_extractor.extract_keywords(grupo['TEXTO'].str.cat(sep=' ')))\n",
        "                  for nome, grupo in GRUPOS}\n",
        "\n",
        "# Obter palavras-chave exclusivas para cada tipo de documento\n",
        "palavras_chave_exclusivas = {}\n",
        "\n",
        "for nome in palavras_chave.keys():\n",
        "    outras_palavras = set().union(*(palavras for outro_nome, palavras in palavras_chave.items() if outro_nome != nome))\n",
        "    palavras_chave_exclusivas[nome] = palavras_chave[nome] - outras_palavras\n",
        "\n",
        "# Ver as palavras-chave exclusivas para cada tipo de documento\n",
        "for tipo_documento, palavras in palavras_chave_exclusivas.items():\n",
        "    print(f'{tipo_documento}: {palavras}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D68zQbdJTzO_",
        "outputId": "e5e6171b-5743-4f59-af6a-3b7d16c67413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CCIR: {'PRIC', 'CADASTRAIS', 'INCLUÍDOS', 'TAXA', 'SERVIÇOS', 'VALOR', 'RURAL', 'IMÓVEL'}\n",
            "CNPJ: {'Município', 'DATA', 'MONTE', 'JURÍDICA', 'ESPECIAL', 'CÓDIGO', 'INSCRIÇÃO', 'SITUAÇÃO', 'CADASTRAL', 'DESCRIÇÃO'}\n",
            "CPF: {'Comprovante', 'Cadastral', 'Receita', 'Brasil', 'Inscrição'}\n",
            "CTRU: {'SANTOS', 'Qtd', 'Dep', 'NAO', 'OQVIJOSSV', 'Rurais', 'Rua', 'Sindicato', 'FRANÇA', 'Trabalhadores'}\n",
            "IE: {'CEP', 'SERVICOS', 'ALUGUEL', 'Endereço', 'Consulta', 'EQUIPAMENTOS', 'CRC'}\n",
            "IRR: {'CODEVASF', 'AQUICULTURA', 'unidade', 'MANIÇOBA', 'Documento', 'CONTRATO', 'IRRIGAÇÃO', 'CLÁUSULA', 'consumidora'}\n",
            "ITR: {'Tipo', 'Nirt', 'DECLARAÇÃO', 'Total', 'SANTANA'}\n",
            "LIAM: {'Lei', 'AMBIENTAL', 'certificado', 'empreendimento', 'Estadual', 'inema', 'Decreto', 'autorização', 'LICENCIAMENTO'}\n",
            "NIS: {'Família', 'AMEASNTT', 'ARA', 'QUEIROS', 'FABIANE', 'Buscar', 'Olá', 'Único', 'PROGRAMAS'}\n",
            "OUTA: {'Art', 'Arts', 'Processo', 'INEMA', 'Declaração', 'qualquer', 'GOVBA', 'uso', 'Outorga'}\n",
            "PRMA: {'UUUZIYL', 'Usuário', 'SAPCANA', 'UUUU', 'LUUZ', 'Açúcar', 'Locumento'}\n",
            "PRONAF: {'PLETSH', 'Nacional', 'MARIA', 'Programa', 'Familiar', 'Apt', 'DAP', 'Minstério', 'Fortalecimento'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paracer sobre o documento"
      ],
      "metadata": {
        "id": "AobmvIJNUOah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parecer utilizando verificação das palavras mais frequentes"
      ],
      "metadata": {
        "id": "Et0eHIvtUobs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar o extrator de palavras-chave\n",
        "kw_extractor = KeywordExtractor(lan='pt', n=1, top=10)\n",
        "\n",
        "# Para cada grupo, concatenar todo o texto e extrair as palavras-chave mais relevantes\n",
        "palavras_chave = {nome: set([keyword for keyword, _ in kw_extractor.extract_keywords(grupo['TEXTO'].str.cat(sep=' '))])\n",
        "                  for nome, grupo in grupos}\n",
        "\n",
        "# Verificar se cada texto contém pelo menos 80% das palavras-chave de seu tipo de documento\n",
        "def verificar_parecer(row):\n",
        "    palavras_texto = set(row['KEYWORDS'])\n",
        "    palavras_relevantes = palavras_chave[row['DECLARADO']]\n",
        "    intersecao = palavras_texto.intersection(palavras_relevantes)\n",
        "\n",
        "    return 'IGUAL' if len(intersecao) >= 0.1 * len(palavras_relevantes) else 'DIFERENTE'\n",
        "\n",
        "# Aplicar a função a cada linha do DataFrame\n",
        "DF['PARECER'] = DF.apply(verificar_parecer, axis=1)\n",
        "#df\n",
        "#df.groupby('PARECER').size()"
      ],
      "metadata": {
        "id": "GRTpNYqbUQBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parecer utilizando modelo SVM"
      ],
      "metadata": {
        "id": "iptqtSWCUe8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar o texto e a coluna \"DECLARADO\"\n",
        "textos = DF[\"TEXTO\"]\n",
        "declarado = DF[\"DECLARADO\"]\n",
        "\n",
        "# Inicializar o vetorizador TF-IDF\n",
        "vetorizador = TfidfVectorizer()\n",
        "\n",
        "# Ajustar o vetorizador aos textos e calcular o TF-IDF\n",
        "vetores_tfidf = vetorizador.fit_transform(textos)\n",
        "\n",
        "# Inicializar o classificador SVM\n",
        "classificador = SVC(probability=True)  # Habilitar o cálculo de probabilidades\n",
        "\n",
        "# Treinar o classificador com os vetores TF-IDF e as classes correspondentes\n",
        "classificador.fit(vetores_tfidf, declarado)\n",
        "\n",
        "# Calcular as previsões do modelo para os textos do DataFrame\n",
        "previsoes = classificador.predict(vetorizador.transform(textos))\n",
        "\n",
        "# Calcular as probabilidades do modelo para cada classe\n",
        "probabilidades = classificador.predict_proba(vetorizador.transform(textos))\n",
        "\n",
        "# Adicionar as colunas \"PARECER\" e \"PROBABILIDADE\" ao DataFrame\n",
        "DF[\"PARECER_SVM\"] = [\"IGUAL\" if pred == decl else \"DIFERENTE\" for pred, decl in zip(previsoes, declarado)]\n",
        "DF[\"PROBABILIDADE\"] = [probs.max() if pred == decl else 1 - probs.max() for probs, pred, decl in zip(probabilidades, previsoes, declarado)]\n",
        "\n",
        "# Obter o índice da classe com a maior probabilidade\n",
        "indice_maior_probabilidade = probabilidades.argmax(axis=1)\n",
        "\n",
        "# Mapear o índice da classe para o nome do documento\n",
        "maior_probabilidade_documento = [classificador.classes_[indice] for indice in indice_maior_probabilidade]\n",
        "\n",
        "# Adicionar a coluna \"PARECER_SVM\" ao DataFrame\n",
        "DF[\"TIPO_SVM\"] = maior_probabilidade_documento"
      ],
      "metadata": {
        "id": "6f5LL4l8Ui4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTES (NÃO CONSIDERAR! SERÁ DELETADO)"
      ],
      "metadata": {
        "id": "pb14zYRcV9EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o caminho do diretório.\n",
        "dir_path = '/content/sample_data/exemplos'\n",
        "\n",
        "# Cria a pasta \"imagem\" se não existir.\n",
        "image_dir = os.path.join(dir_path, 'imagem')\n",
        "if not os.path.exists(image_dir):\n",
        "    os.makedirs(image_dir)\n",
        "\n",
        "# Lista todos os arquivos no diretório.\n",
        "files = os.listdir(dir_path)\n",
        "\n",
        "# Filtra apenas os arquivos PDF.\n",
        "pdf_files = [file for file in files if file.endswith('.pdf')\\\n",
        "             or file.endswith('.PDF')]\n",
        "\n",
        "# Função para converter e salvar imagens JPEG.\n",
        "def convert_pdf_to_jpeg(pdf_path):\n",
        "    # Converte pdf para jpg.\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    # Pega o nome do arquivo em formato .pdf sem a extensão.\n",
        "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "\n",
        "    # Salva a imagem em formato .jpeg.\n",
        "    for i, image in enumerate(images):\n",
        "        image_path = os.path.join(image_dir, f'{base_name}_{i}.jpeg')\n",
        "        image.save(image_path, format='png')\n",
        "\n",
        "# Converte e salva todas as imagens.\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(dir_path, pdf_file)\n",
        "    convert_pdf_to_jpeg(pdf_path)"
      ],
      "metadata": {
        "id": "G774K495WIuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/sample_data/exemplos/imagem/1000112647_CTRU_0.jpeg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "texto = pytesseract.image_to_string(gray, lang='por')\n",
        "print(texto)"
      ],
      "metadata": {
        "id": "QmXXeMhTV-Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "id": "57mLnW79kqvS",
        "outputId": "0581f35c-82b6-4f02-8b4f-3ac2f3df0c03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n"
          ]
        }
      ]
    }
  ]
}